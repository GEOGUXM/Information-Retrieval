http://www-theory.fnal.gov/tev33.ps

   The TEV33 Committee Report Dan Amidei, William Bardeen, Giorgio
   Bellettini, Raymond Brock, Joel Butler, Bill Carithers, Roger Dixon,
   Estia Eichten, Dave Finley, Paul Grannis, Chris Hill, Joe Incandela,
   Dave McGinnis, John Marriner, Hugh Montgomery, Tom Nash, John Peoples,
   Chris Quigg, Ken Stanfield, Paul Tipton, Alvin Tollestrup, John
   Womersley, Darien Wood Executive Summary The Tevatron Collider is the
   highest-energy machine in the world. Run-II, with a planned integrated
   luminosity of a 2 fb\Gamma 1, offers a rich menu of search and
   measurement. The Tevatron can remain the world's premier discovery
   machine until the LHC becomes productive, provided that increases in
   useful luminosity make possible meaningful steps in sensitivity. We
   expect that after the LHC comes into operation, the Tevatron collider
   will be competitive only in specialized studies. While the Tevatron
   defines the energy frontier, there is a clear premium in luminosity
   increases beyond what has been achieved thus far. Up to an integrated
   luminosity of , 100 fb\Gamma 1, there are identifiable benefits to each
   step in luminosity. A prime example of the benefits of increased
   sensitivity is the discovery potential for an intermediate-mass
   Higgs-boson in the W \Sigma H and Z0H final states. Here the Higgs
   decay to bb can be observed, which is essential to confirm the mass
   giving attribute of the Higgs-boson in the Standard Model. While the
   LHC is sensitive to the rare H ! flfl mode, the Tevatron affords a
   signal/background in the dominant H ! bb mode that is significantly
   better than that at the LHC. Another important example is the study of
   the reaction qq ! W ! tb. This allows precision measurement of Vtb in
   the Standard Model. It also probes the W tb vertex at a timelike Q2 ,
   m2t which is potentially sensitive to new physics beyond the Standard
   Model. This reaction can optimally be studied in a high luminosity
   Tevatron where backgrounds from competing subprocesses are small.
   Higher useful luminosity also benefits precision measurements of
   crucial parameters of the electroweak theory. These include measurement
   of the top quark mass to a precision of , 1 GeV, and the W -boson mass
   to a precision of , 15 MeV. These measurements will be of central
   importance in testing the Standard Model consistency with the
   prediction of the mass of the Higgs-boson. While extending the
   discovery limits for supersymmetric particles, avatars of electroweak
   symmetry breaking that decay into t_t, and the like, luminosity
   improve1 February 1996 ments will aid the crucial task of examining the
   characteristics of top production and decay in detail. Exotic species
   like the Bc meson and b-baryons should come more clearly into view, and
   the Tevatron's capability to explore Bs- _Bs mixing and to probe CP
   violation will be enhanced. From the analysis presented in the TeV2000
   studies, we believe that a strong scientific case can be made for a run
   of approximately 30 fb\Gamma 1 completed before the LHC hits its
   stride. Specifically, we propose to set a target of accumulating 30
   fb\Gamma 1 of useful events before the end of 2006. To judge whether
   this is a cause Fermilab should embrace, we need to understand
   thoroughly the scope of effort and cost required to make it happen.
   Some specific questions to guide an integrated optimization of the
   collider complex and detector(s) are as follows: ffl What is the
   optimal way to raise the peak luminosity to 1 to 2 \Theta 1033 cm\Gamma
   2 s\Gamma 1? ffl How can the Main Injector intensity be utilized most
   effectively to satisfy the needs of future fixed target experiments
   (such as NUMI) and high luminosity demands on the antiproton source?
   ffl Is it practical to operate at lower peak luminosity, say 5 \Theta
   1032 cm\Gamma 2 s\Gamma 1, but with "level" luminosity, to reduce
   demands on the detectors for the desired integrated luminosity? ffl
   What are the trade-offs in number of useful events to tape? ffl What is
   the highest luminosity at which the Run-II CDF and DO/ detectors and
   subsystems retain their key physics capabilities? ffl What is the
   largest integrated luminosity that could be accumulated by these
   detectors by the end of 2006? ffl What is the shortest bunch spacing at
   which the Run-II detectors can operate? What is the shortest bunch
   spacing at which the collider can operate? ffl What additional upgrades
   would be required for the detectors to operate (i) at a constant
   luminosity of 5 \Theta 1032 cm\Gamma 2 s\Gamma 1? (ii) at a peak
   luminosity of 1 - 2 \Theta 1033 cm\Gamma 2 s\Gamma 1? Once answers to
   these questions and a funding scenario become available, it will be
   possible to devise a strategy best suitable to achieve the optimum
   physics outcome from the Tevatron collider. The committee recommends
   that, immediately, a significant effort be set in motion to address the
   physics and detector issues of a 30 fb\Gamma 1 program on the timescale
   of 2 Snowmass `96. The studies should extend the TeV2000 report to
   incorporate detailed simulations relevant to detector performance in
   the high luminosity environment and based upon real Tevatron data
   wherever available. Working groups that take advantage of the existing
   expertise within the CDF and DO/ collaborations should be formed to
   establish detector performance at high luminosity including tracking,
   calorimetry, lepton coverage and identification, triggering and data
   acquisition. A decision to proceed toward a design study will require a
   dedicated R&D effort to develop technologies for the high luminosity
   environment. A major decision will have to be taken concerning whether
   one or two detectors be upgraded for high-pT physics at TeV33. This
   decision will have to take into account the availability of funding,
   limits on available manpower, and the potential desirability of having
   a free interaction region (e.g., for a B-physics detector). The
   committee recommends an urgent study of the accelerator improvements
   required to reach the luminosities needed for a 30 fb\Gamma 1 physics
   program. This study should develop realistic solutions which are both
   cost effective and do not require substantial delays in the physics
   program. Innovative solutions that enhance machine performance or
   reduce cost should be fully explored. It is very important that there
   be coordinated effort between the detector and accelerator development
   studies, e.g., addressing such issues as bunch spacing or average vs.
   peak luminosity, etc. To focus the Laboratory's efforts to answer these
   questions, we recommend that a Workshop on Physics at a High-Luminosity
   Tevatron Collider be organized at Fermilab in late April or early May.
   The charge of the workshop would be to explore practical strategies for
   an integrated approach to the machine and detector(s). A draft program
   for a two-day workshop is appended to this report. We recommend that a
   small team be charged with the responsibility of coordinating the
   Laboratory's efforts through the DPF Summer Study, including the
   organization of the Fermilab workshop. We suggest that the Director
   review and implement mechanisms by which the TeV33 program, if
   approved, can be accelerated. 3 Introduction The TeV33 Committee has
   considered the physics potential of a high luminosity Tevatron collider
   program following Run-II. In the following sections we discuss the
   physics opportunities associated with an integrated pp luminosity of 30
   fb\Gamma 1 including issues associated with upgrades to the collider
   detectors and the accelerator complex which would be required to
   exploit this physics. Many of the physics issues discussed in this
   report are the focus of the TeV2000 Study Group on Future Electroweak
   Physics at the Tevatron, as presented in the TeV2000 Report, edited by
   R. Brock and D. Amidei. The committee has benefited from a series of
   Fermilab Special Seminars including presentations by members of the
   TeV2000 Study Group and the Fermilab Accelerator Division. We review
   the essential elements of these studies in the following sections,
   which provide a strong scientific case for a high luminosity physics
   program at the Tevatron collider. 1. Top Quark Physics The TeV2000
   Study Group presented the physics potential of large top event samples.
   Detector performance was assumed to be comparable to that anticipated
   for DO/ and CDF in Run-II. Event yields (per pb\Gamma 1) are thus more
   than twice the current Run-I yields as a result of increased
   acceptances and efficiencies, as well as the increased center of mass
   energy. Expectations for Run-II (, 2 fb) and for data sets of 10 and 30
   fb\Gamma 1 are summarized in Table 1. For the top mass measurement via
   kinematic fits to lepton + jets events the statistical uncertainty
   varies as ffimt , 25 GeV=pN and hence will be at the 1 GeV level by the
   end of Run-II. The systematic uncertainty derives predominantly from
   jet energy corrections which are necessitated by instrumental and
   physics effects both of which are controlled with data. It is therefore
   expected that all systematic uncertainties will be inversely
   proportional to the square root of the number of top events as shown in
   Table 2. Other methods for determining mt include kinematic mass fits
   in dilepton and all-hadronic event samples and a fit to the mean B
   decay length in top events. These methods are statistically
   independent; the decay length method is also independent of jet energy
   scale. It is reasonable to expect that these tools will allow a
   determination of mt to better than 4 GeV per experiment in Run-II and ,
   1 GeV for data samples of 10 fb\Gamma 1 . For Standard Model
   constraints on the Higgs-boson mass, greater precision may be
   superfluous. Regarding tt production, the uncertainty on oett in Run-II
   arises from acceptance (6%) , backgrounds (7%) and luminosity (3:5%).
   The latter is determined by the effective cross section of the
   luminosity monitors while the others should fall as 4 Table 1:
   Expectations for top parameters tt Parameter Run-II (2 fb\Gamma 1 ) 10
   fb\Gamma 1 30 fb\Gamma 1 N(W+*3 Jets)+b tag 1170 6800 17600 N(W+ 4
   Jets)+2b tags 520 2600 7800 ffimt ^4 GeV ,1 GeV !1 GeV ffioett =oett 8
   % 3% 3% ffi(oe``=oe`+jets) 10 % 5.5% 4.0% ffi(oe \Theta B(X ! tt )) 70
   fb 20 fb 10 fb ffiB(t ! W b) 3% 1% , 0:5% ffiB(t ! W(V +A)b) * 1:5% *
   0:5% * 0:2% ffiB(t ! Wob) , 3:0% , 1:0% , 0:5 Vtb *0.2 *0.4 * 0:5 B(t !
   cfl) ! 2 \Theta 10\Gamma 3 ! 4 \Theta 10\Gamma 4 ! 1 \Theta 10\Gamma 4
   B(t ! cZ) ! 2 \Theta 10\Gamma 2 ! 4 \Theta 10\Gamma 3 ! 1 \Theta
   10\Gamma 3 B(t ! cfl) ! 3 \Theta 10\Gamma 3 ! 5 \Theta 10\Gamma 4 ! 2
   \Theta 10\Gamma 4 B(t ! H+b) ! 16% ! 8% ! 5% Single top parameter
   ffi\Gamma 13% 6% 5% 95% CL Range Vtb (0.88,1.10) (0.97,1.03)
   (0.98,1.015) the square root of the number of events. The luminosity
   uncertainty cancels in the ratio of dilepton to lepton + jets cross
   sections which is sensitive to non-Standard Model decays such as t !
   H+b, and t ! ~O/~t. With large data sets it will also be possible to
   probe for resonances in the tt channel that are anticipated in some
   models of electroweak symmetry breaking. The ratio of longitudinal to
   left polarized W 's from top decay is fixed in the Standard Model by
   the relationship ffo=ff\Gamma = m2t =(2 M 2W ). With large data samples
   one can test this expectation and search for anomalous right-handed
   couplings by fitting to helicity angle measurement distributions.
   Estimates for resolution of ffo in 2, 10 and 30 fb\Gamma 1 are , 3:3,
   1.5, 1.0 % and , 1:3, 0.6, 0.3 % for an anomalous right-handed
   component ff+. However, these estimates may be optimistic since they do
   not take into account jet-parton misassignments and energy scale
   uncertainties. From careful accounting of double tagged, single tagged,
   and untagged lepton + jets data and tagged versus untagged dilepton
   data it is possible to measure the branching fraction b = B(t ! W b)
   which measures Vtb, and be sensitive to anomalously low values of this
   matrix element. Also shown in Table I are the anticipated sensitivities
   to rare decays of the top such as t ! cfl, cZ, W s and H+b via direct
   searches. of data. In the Standard Model, one would not expect to see
   the 5 Table 2: Systematic uncertainties in mt . Systematic Run-I Run-II
   10 fb\Gamma 1 30 fb\Gamma 1 Jet Scale QCD 7.7 1.3 0.6 0.3 Jet Scale
   Calorimeter 3.1 0.6 0.3 0.1 Total Jet Scale 8.3 1.4 0.7 0.3 b Tagging
   bias 2.4 0.4 0.2 0.1 Background Shape 1.6 1.6 0.7 0.4 Fitting technique
   3.1 - - - MC Statistics 3.1 - - - Total 10 2.2 1.0 0.5 other modes at
   the Tevatron. Since top decays prior to hadronization, the spin
   correlations induced in production lead to additional correlations in
   the decay product distributions. Further information about the
   structure of the fundamental couplings can be obtained from these
   correlations. These correlations are sensitive to the mix of
   subprocesses involved in production, i.e., correlations seen at the
   Tevatron or LHC are unique to each machine (the Tevatron probes
   primarily qq ! g ! tt, while LHC probes gg ! tt). Finally, with large
   integrated luminosity it will be possible to isolate samples of single
   top events produced predominantly via s-channel virtual W decay, with
   an additional , 25% from t-channel pp ! tb + X. Since the s-channel
   cross section for single top production is directly proportional to
   \Gamma (t ! bW ), it is sensitive to large values of Vtb in the
   Standard Model. The significance of the precision measurement of Vtb is
   about 2% with 30 fb\Gamma 1. Combining \Gamma (t ! bW ) with B(t ! bW )
   from tt one can extract the total top width. Moreover, single top
   production in the qq ! W ! tb mode is potentially sensitive to a large
   form factor in the W tb vertex at Q2 = m2t . Since the LEP experiments
   see a significant excess in Rb, the branching ratio for Z ! bb, it is
   interesting to look for new physics in the W \Lambda ! tb channel. The
   fundamental parton subprocess process, qq ! W ! tb, is less accessible
   at either the LHC or the NLC. In conclusion, large integrated
   luminosity data samples at the Tevatron would enable numerous precision
   top quark measurements and tests. For some of these measurements, such
   as mt , 10 fb\Gamma 1 may be adequate. For others, such as the
   determination of Vtb from single top, limits on rare decays,
   polarization measurements, final state correlations, etc., there is a
   more meaningful gain from a larger 30 fb\Gamma 1 data set. Many of the
   measurements discussed here can be done as precisely or even more
   precisely at LHC and/or NLC. Clear exceptions are the study of single
   top production, from which it is possible to obtain a more precise
   measurement of 6 \Gamma (t ! bW ), and qq ! tt, for which LHC will be
   fairly insensitive to s-channel production which dominates at the
   Tevatron. 2. Electroweak Physics In 30 fb\Gamma 1, over 3 \Theta 107
   leptonic W decays and 3 \Theta 106 leptonic Z decays will be recorded
   and available for use in a measurement of the W mass and width. The
   present experience is that the statistical uncertainty in the W mass
   measurement decreases as q1=N , where N is the number of observed W
   events. However, as luminosity increases, detailed Monte Carlo studies
   predict a degradation in transverse mass resolution due to multiple
   interactions. This effect scales like pIc, where Ic is the mean number
   of interactions per crossing. Current theoretical uncertainty
   contributing to the W mass measurement is of order 20 MeV/c2. Recent
   calculations of radiative W and Z production and decay should decrease
   this by at least a factor of two. The remaining theoretical uncertainty
   due to higher order QED corrections that have not yet been calculated
   will likely be reduced once these calculations are performed. It is
   expected that all the known experimental systematic uncertainties in
   the W mass will decrease with increased W sample size. Of particular
   concern is the uncertainty due to parton distribution functions
   (PDF's). Until recently, PDF's have been constrained almost exclusively
   using non-collider data. However, the collider W sample is now being
   used to study the u\Gamma to\Gamma d quark ratio via the lepton
   asymmetry. Asymmetries in (Z; fl) ! e+e\Gamma will also give tight
   constraints on light quark PDF's. The u \Gamma to \Gamma d ratio is the
   most important parameter in the PDF's as far as the W mass measurement
   is concerned. Thus the uncertainty on the W mass due to PDF's is also
   expected to scale as q1=N . With 30 fb\Gamma 1 a total uncertainty on
   the W mass of less than 15 MeV/c2 is predicted. This is a factor of
   four better than one expects for LEP II, and a factor of six better
   than anticipated for Run-II. An NLC experiment would not be competitive
   in measuring the W mass. The uncertainty in the W width is predicted to
   be 15 MeV/c2, an order of magnitude better than expected at LEP II and
   the current measurements. Assuming the Standard Model, when combined
   with a 1 GeV/c2 top mass determination, the W mass measurement leads to
   a prediction of the Higgs mass to 30% of itself. It should be noted
   that there is no known reason why the W mass uncertainty will not
   continue to decrease with increased statistics. One should keep in mind
   that these expectations have been derived by extrapolating almost three
   orders or magnitude from Run 1A results. One important additional
   assumption is that no new systematic effects will be uncovered that
   domi7 nate the uncertainty. Additional Monte Carlo studies that perform
   the entire W mass analysis on realistic Monte Carlo data samples is an
   important next step. One potential concern is the effect of added
   material in upgraded tracking systems. Feedback from these studies into
   the detector design is essential. It should also be noted that given
   the increased event complexity and triggering difficulties at the LHC,
   it is likely that the most precise measurement of the W mass will be
   from Fermilab's Tevatron. Determining the W mass is a worthy physics
   goal for the Tevatron, even during the LHC era. Also expected in 30
   fb\Gamma 1 are about 1500 W W and 600 W Z events with leptonic final
   states. This assumes Standard Model tri-boson couplings. It is common
   to define effective Lagrangians for W W V (V = Z or fl) and ZflV
   couplings. Deviations from the Standard Model expectations for these
   couplings would be a signal of exciting new physics. Current limits on
   these coupling parameters from DO/ and CDF allow a typical range from
   -1 to 1 at 95% confidence level. In 30 fb\Gamma 1, limits on W W Z or W
   W fl couplings can be improved by a factor of ten to twenty. The ZZfl
   and Zflfl couplings will be improved by factors of 10 to 100. In the
   former case this is competitive with LEP II, in the latter, it is
   factors of 2 to 20 better than LEPII. It should be noted, however, that
   limits on these couplings from LHC with 100 fb\Gamma 1 should be
   factors of 10 to 100 better than TeV33. The precision measurement of
   sin2(`W ) by LEP and SLC has been one of the most important physics
   results of the last decade. The high statistics p_p ! `+`\Gamma sample
   provided by the Tevatron can also be used to measure sin2(`W ). The
   systematic uncertainty on sin2(`W ) from the 1988-89 CDF data was
   0:002. The current LEP/SLC uncertainty is 0:00028. With 30 fb\Gamma 1
   the estimated uncertainty at the Tevatron will be 0:00021. This
   provides a significant measurement complimenting the LEP and SLD
   values. Additional interesting physics potential exists in the the
   search for the radiation zero in W fl final states, expected in the
   Standard Model and absent in many non-standard W W fl couplings. The
   effect is much more dramatic at Tevatron energies than it will be at
   the LHC. Finally, the search for CP violation in leptonic W decays is
   very interesting but will place great demands on the detector. 3. Light
   Higgs Physics Understanding the origin of electroweak symmetry
   breaking, i.e., the Higgs sector, is the next step for particle
   physics. The Tevatron with high luminosity can study the associated
   production process q _q ! (W; Z) ! (W; Z) + H with H ! b_b up to a mass
   of order , 140 GeV, depending upon integrated luminosity. This process
   is essential to confirm that the Higgs-boson really is associated with
   fermionic mass 8 generation, as predicted in the Standard Model. In the
   Standard Model we know that the Higgs mass is bounded within a range:
   65 GeV ! mH ! 1 TeV. In any Grand Unified Theory containing a desert mH
   ! 200 GeV, while in the Minimal Supersymmetric Standard Model (MSSM) mH
   ! 130 GeV; it is claimed that mH ! 150 GeV in any SUSY scheme.
   Precision electroweak studies do not limit mH but somewhat prefer light
   Higgs. LEP I has mapped the Higgs mass up to 65 GeV; LEP II can reach
   to 85 GeV (ps = 184 GeV) and 95 GeV (ps = 192 GeV) in e+e\Gamma ! Z ! H
   + Z. LHC (ATLAS) claims H ! flfl is visible for 110 ! mH at 100
   fb\Gamma 1, or 80 ! mH at 500 fb\Gamma 1. H ! flfl is, however, a very
   difficult measurement at the LHC. We note, however, that the NLC could
   cover the full mass range in this mode as well. In q _q ! W ! W + H
   with H ! b_b, the dominant background is W + 2j. This is reducible with
   single b-tagging and much reduced with double b-tagging. The CDF
   efficiency of 50% has been assumed for study purposes. At
   next-to-leading order, W +H production yields about a 1 pb total
   cross-section. The dominant backgrounds are (i) W + bb, (ii) W + (Z !
   bb), (iii) t_t ! W bW b missing a W , (iv) W \Lambda ! tb, and (v) W +
   g ! j + W + bb. These backgrounds are believed to be understood to (i)
   \Sigma 30% and (ii) \Sigma 20%. Others are small and irrelevant up to
   mH , 120 GeV. The biggest feasibility issue is jet energy resolution.
   Final-state gluon radiation broadens the distributions at small pT . In
   the Tevatron light Higgs simulation studies it was assumed that the jet
   cluster algorithm can be improved reducing the width of the
   distribution. A second major issue is cuts: (i) both b jets with pT ?
   15 and jyj ! 2:0, (ii) typical W cuts (iii) no extra jets with pT ? 30
   Gev, no 2 extra jets with pT ? 15 GeV. Optimized cuts use b_b cms
   angles, MW bb, and arguably one can get about a 4\Theta optimization of
   signal to background. We assume presently that a factor of two
   optimization is achievable in the light Higgs search. The results of
   the detailed study are as follows. For 10 fb\Gamma 1, (50% efficiency
   for b-tag, 2\Theta , per expt.), one finds mH = 60 GeV, oe = 9:4; to mH
   = 120 GeV, oe = 3:3. A 5oe discovery potential has been estimated for
   mH = 120 GeV (mH = 140 GeV) with 25 (100) fb\Gamma 1. Further detailed
   study at the higher masses is, however, required. A study comparison
   gave ratios, R, of Signal/Background for Tevatron/LHC: for mH = 60, R =
   7:0; mH = 120 R = 9:5. Thus S=B is expected to be better at the
   Tevatron. This is attributed to the fact that large new backgrounds
   emerge at the LHC, mainly due to t_t and t_b production. Therefore
   Higgs detection at the high luminosity Tevatron in associated W H, ZH
   production appears to be possible. The high efficiency of b tagging may
   be a virtue of this machine. However, we need a smarter jet clustering
   algorithm, and for mH ? 120 GeV we need to integrate more than 30
   fb\Gamma 1. We note that the associated Z + H is a good channel, but
   the rate with Z ! e+e\Gamma or Z ! _+_\Gamma is marginal. This process
   becomes more interesting if one can trigger on the ** decay modes of
   the Z0. One MC simulation for ET ? 35 GeV obtained 85% efficiency, but
   a background 9 study is required here. The W (jj) + H(! o/ o/ ) process
   has also been considered. The oeB = 9:4 fb, and the dominant background
   is Z(o/ o/ )+jj with a gigantic cross-section oe = 1:8\Theta 105 fb. In
   simulations one reconstructs the o/ using missing ET projected along
   the lepton axes. Unfortunately the Z tail seems to be a killer for mH !
   120 GeV. For example, one finds with 100 fb\Gamma 1 S = 107 events and
   Z + jj = 940 events. To be able to exploit this channel an improved
   missing ET resolution of about a factor of 2 than the present
   capability will be needed, or an excessive coupling of Ho/ o/ with
   respect to Standard Model must be present. Another possibility is to
   study Z(`+`\Gamma ) + H(o/ o/ ) and the Z + Z background is eliminated.
   This has no background, but occurs at a much lower rate. If the
   Higgs-boson is very light and is seen at LEPII, then the HZZ coupling
   is measured. How important is knowledge of HW W coupling? In
   multi-doublet schemes HW W=HZZ is fixed to the Standard Model value.
   Measuring this ratio will confirm the Standard Model nature of the new
   particle. On the other hand one can get unusual ratios with
   isotriplets, etc. The ratio is different in SUSY because the job of
   electroweak symmetry breaking is shared amongst several Higgs fields,
   making the rate in the light Higgs channel smaller. With a Higgs
   discovery in hand it ultimately becomes a question of precision study
   of both HW W and HZZ. An 80 - 110 GEV Higgs-boson is an optimistic
   scenario with which one could do a lot of physics. The precision
   studies of such an object at the high luminosity Tevatron, assuming it
   was already seen in LEPII or Run-II, has not yet been considered. Only
   associated production has been considered here. For example, gluon
   fusion production exploitable with H ! ZZ or W W may be observable at
   the Tevatron for larger-mass Higgs-bosons and high luminosity. It may
   be worth studying W W \Lambda in different channels like flavor tagged
   ones such as ` + charm + strange. 4. Physics Beyond the Standard Model
   Despite the great successes of the Standard Model there remain strong
   theoretical reasons to believe that it is not complete. The Standard
   Model is unnatural (and requires fine tuning) at scales much above the
   electroweak symmetry breaking scale. This can be rectified by adding
   new particles and interactions at nearby scales. This can be
   accomplished by supersymmetric extension of the Standard Model (MSSM)
   or by introducing new dynamics (e.g. technicolor, topcolor, or
   compositeness). Some of the specific questions that remain to be
   answered are: Is there an elementary Higgs-boson? If so, is there only
   one scalar-doublet? (Both MSSM and new dynamics expect more.) The top
   mass is as large as the electroweak scale. Is the interaction that
   generates the top quark mass related to the scale of electroweak 10
   symmetry breaking? If it is related, what more does this tell us about
   the origins of quark and lepton masses and mixings? With the integrated
   luminosity to date the Tevatron program has reached only the lowest
   edge of the mass scale where this new physics might appear. Although
   full exploration will require at least the LHC, we can extend this
   discovery potential well into this region for new physics with Tevatron
   accumulations of 30 fb\Gamma 1 . 4.1 Supersymmetry (SUSY) The minimal
   supersymmetric standard model (MSSM) is well studied and theoretically
   attractive. It avoids the fine tuning problem for the Higgs-boson mass
   and associates the large top quark mass with the scale of electroweak
   symmetry breaking. It predicts a spectrum of superpartner masses which
   gives the TeV33 a large discovery window. In the MSSM, the various
   superpartners have the following discovery potential: ffl Squarks and
   Gluinos: The classic signature is jets and missing ET . Mass limits
   approach , 300 GeV for 2fb\Gamma 1 and , 500 GeV for 30fb\Gamma 1 .
   Preliminary studies of the effects of multiple interactions using CDF
   and DO/ events look encouraging. ffl Charginos and neutralinos: Here
   the signature that has extensively studied in the Tev2000 report is
   three leptons and missing ET . With 30fb\Gamma 1 the lightest chargino
   can be discovered up to 250 GeV, covering a significant fraction of the
   parameter space. ffl Light stop: The heavy top quark implies a light
   stop. The signatures depend on mass heirarchy. Possible decays are ew +
   b, ez + c, W + ez + b, b + l + e*, and b + e` + nu (where ~x denotes a
   superpartner). The branching ratios are expected to be 5 to 10%. With
   2fb\Gamma 1 we can probe down to 6%, while with 30fb\Gamma 1 we can
   cover down to 3% which includes the whole expected range. The basic
   conclusion is that, while the Tevatron cannot cover all of the SUSY
   parameter space, it can explore a very significant fraction of it. To
   support that conclusion the existing study should strenghten three
   areas. First, there should be detailed studies of the effect of
   multiple interactions on all discovery limits. Second, there should be
   more complete studies of alternative decay modes and associated
   discovery limits. Third, assuming discovery, there should be study of
   how well the masses and properties of the superpartners can be
   measured. 11 4.2 New Interactions It is possible that electroweak
   symmetry breaking arises from some new strong dynamics (e.g.,
   technicolor or topcolor). Then the expected discoveries would include
   new particles and new interactions. The lowest mass particles would be
   (pseudo)scalars (technipions, leptoquarks, etc.) with masses in the
   100-200 GeV range. In addition the new dynamics may affect the
   production and decay properties of the quarks and electroweak bosons.
   The details are model dependent and there is at present no generally
   accepted Standard Model. However some general features of these models
   emerge: ffl Low-lying spin-zero states couple in flavor-sensitive ways.
   Usually, their coupling to fermion pairs is proportional to mass. Hence
   heavy-flavor tagging is important to studying this physics. ffl Some
   may also couple significantly to gauge boson pairs. Hence events with
   one or more electroweak bosons in the final state are particularly
   sensitive to searches for new dynamics. ffl Low-lying spin-one states
   couple in flavor blind ways. ffl The new dynamics may produce
   resonances in production cross sections for ordinary quarks. The third
   generation quarks, t and b, may be particularly sensitive to new
   physics. A number of specific exotic particles were studied in the
   TeV2000 document which showed very substantial increase in reach with
   increased luminosity. The mass limits are increased by 30 \Gamma 50%
   between Run-II and TeV33. The limits on compositeness are extended to
   2.6 TeV for the q _qq _q interaction and 7.0 TeV for the q _qe+e\Gamma
   interaction. 4.3 B Physics The detector envisioned in this report for
   the TeV33 environment is not an optimized B detector. However the
   requirement for excellent tagging of heavy flavors for top, Higgs, and
   new physics searches allow a substantial amount of new B physics to be
   done. In Run-II, high priority will be given to measurement of the CKM
   unitarity angles. Sensitivity to sin 2fi in Bd ! Ks is expected to be
   oesin 2fi ^ 0:15 (the Standard Model predicts sin 2fi = 0:65 \Sigma
   0:12). B ! ss+ss\Gamma events, accumulated via 12 a hadronic B decay
   trigger, will be used to attempt a measurement of sin 2ff. The angle fl
   may be measured in untagged Bs decays if there is a large difference in
   the widths of the heavy and light mass eigenstates, or via flavor
   tagging of Ds`* and Dsnss final states. The width difference could be
   used to set a lower limit on the Bs mixing parameter, xs, which will
   also be measured directly in time-dependent, flavor tagged studies of
   Bs \Gamma Bs mixing. Finally, larger data sets could result in the
   first observation of the rare decays B+ ! _+_\Gamma K+, Bo ! _+_\Gamma
   K\Lambda o, the discovery of the Bc via Bc ! ss, and accumulation of a
   large sample of \Lambda b events. While this program is impressive, it
   should be noted that all of these measurements will be statistically
   limited at the end of Run-II. Of the CKM unitarity angles, for
   instance, only for sin 2fi are we relatively certain to have adequate
   statistics. Similarly, it is unlikely that the Run-II data sample will
   be adequate to reach instrumental limitations in the determination of
   xs. An order of magnitude or more data, assuming no major degradation
   in flavor tagging and reconstruction efficiencies, would greatly
   facilitate measurement of the angles ff and fl while fully exploiting
   detector capability for xs. Of course sensitivity to rare states and
   rare decay modes will also increase. For 30fb\Gamma 1 of data, we
   expect several thousand reconstructed Bc events which could enable
   studies of the rich spectrum of excited Bc states. We would expect to
   accumulate , 104 \Lambda b's and, hence, should become sensitive to a
   broad range of b baryons. Collection of significant samples of the rare
   decays B+ ! _+_\Gamma K\Lambda o and B+ ! _+_\Gamma K+ would also be
   possible and one would expect to just begin to observe Bs ! _+_\Gamma
   assuming the Standard Model predictions for its production. In
   conclusion, there exists enormous potential for significant B physics
   results beyond Run-II. Furthermore, virtually all of the measurements
   mentioned above are intrinsic to hadron colliders and could not be made
   at planned or existing facilities prior to the operation of LHC. 5.
   Detector Issues The detector (or detectors) to be used for high
   transverse momentum physics at the upgraded Tevatron will certainly
   require improvements over the existing CDF and DO/ detectors. The main
   challenge will be to maintain roughly the present level of performance
   in a much harsher environment, with up to 10 interactions per crossing,
   rather than to improve (for example) the calorimeter resolution or to
   trigger on new processes. The TeV2000 report provides a first look at a
   number of detector issues. None is seen as being a "show-stopper" but
   all are generally agreed to require some more realistic and thorough
   study. Regarding tracking, a silicon detector providing b-tagging will
   be an essential requirement. Tagging efficiencies and light-jet
   rejections similar to those already obtained by CDF appear to be
   adequate for the physics of TeV33, but maintaining 13 such performance
   in the higher luminosity environment will be a challenge. The LHC
   experiments have chosen to address this problem with pixel arrays, but
   this is a potentially expensive solution. For hard physics, muon and
   electron detection over jjj !, 2\Gamma 2:5 will be required with
   isolated muon and electron triggers at moderate momenta (p`T ?,
   15\Gamma 20 GeV/c). For accurate B-physics studies, a lower threshold
   of order 5 GeV would be needed. Precision EM energy resolution (better
   than the typical , 15%=pE) does not seem to be warranted. Some loss of
   efficiency will occur for lepton isolation cuts in the high-luminosity
   environment; initial studies suggest that this is manageable. Hadronic
   calorimetry is required to reconstruct both jets and missing transverse
   energy (E/T ). Segmentation finer than \Delta j \Theta \Delta OE = 0:1
   \Theta 0:1, and energy resolution better than the typical , 70%=pE, do
   not seem to be warranted. This is because the limit on multi-jet mass
   resolution comes from gluon radiation rather than from detector
   effects. One would have to optimize the jet clustering and clean-up
   criteria. Initial studies indicate that this is possible, but how to
   best optimize this resolution deserves further study as it is critical
   to the light Higgs search. The effect of multiple interactions on E/T
   has been studied with limited statistics and does not seem to increase
   the number of high-E/T events (E/T * 40 GeV) significantly. A trigger
   and data acquistion system capable of recording at least the inclusive
   leptonic W and Z rates (and hence associated Higgs), the top to
   leptons, and inclusive E/T +jets events, is required. A first look
   suggests that this is feasible but would require significant bandwidth
   enhancements over the Run-II CDF or DO/ configurations. How to handle
   the large stream of data also needs much thought -- a quasi-real-time
   reconstruction facility is one option. The systematics in the W and top
   mass measurements also deserve some further study. It is important to
   the physics case for TeV33 that there is no significant systematic
   error that might prevent the results improving essentially as 1=pN .
   Measurements performed to date have not yet indicated any such
   systematic error. The whole issue of radiation damage and shielding has
   not been addressed in the TeV2000 study. This is potentially very
   important, both for survival of detector components and for the ability
   to trigger (especially for muons). Considerable expertise exists at
   Fermilab and this should be brought to bear on the problem. The
   committee therefore recommends that a significant effort be set in
   motion to address these detector issues on the timescale of Snowmass
   '96. This effort should fully employ the large amount of expertise
   available within the CDF and DO/ collaborations. The studies should
   involve detailed simulations and the use of real Tevatron data wherever
   available. On a slightly longer timescale, a detector R&D effort should
   be initiated to develop the needed technology for the TeV33
   environment. It is clear that the latter can contribute to, and benefit
   from, LHC detector R&D. 14 6. Accelerator Issues In order to achieve
   peak Tevatron Collider luminosities beyond the planned Run-II
   luminosity of 2 \Theta 1032 cm\Gamma 2sec\Gamma 1, the accelerator
   complex must be upgraded. With the new recycler ring, the antiproton
   production rate becomes the most important limitation in the achievable
   luminosity. The average luminosity times the protonantiproton
   cross-section equals the stacking rate times the fraction of
   antiprotons lost through beam-beam interactions. This fraction is
   difficult to predict accurately but may approach unity because of the
   increased aperture of the Main Injector (compared to the Main Ring) and
   because of recapture in the Recycler Ring of the antiprotons that
   remain at the end of the store. In the limit that the fraction becomes
   unity, an average luminosity of 1033 cm\Gamma 2sec\Gamma 1 requires a
   stacking rate of 50 \Theta 1010 per hour. The antiproton source can now
   stack 7 \Theta 1010 antiprotons per hour; the projected rate for Run-II
   is 20 \Theta 1010 antiprotons per hour. Thus, a substantial upgrade of
   the antiproton source is required in order to achieve a luminosity of
   1033 cm\Gamma 2sec\Gamma 1. It appears to be straightforward to
   increase the luminosity to 1033 provided that an increased antiproton
   stacking rate be achieved along with correspondingly larger, somewhat
   more dense, antiproton stack. The luminosity is proportional the number
   of bunches times the number of antiprotons per bunch. If the increased
   antiproton stack is distributed among more antiproton bunches with a
   higher intensity per bunch, the luminosity goal of 1033 can be met or
   exceeded. The number of bunches, and therefore the luminosity, can be
   increased by approximately a factor of 3 if the bunch spacing is
   decreased to 7 buckets (132 nsec) and the number of particles per bunch
   is held constant. The feasibility of the 7 bucket spacing is supported
   by the work reported in TM-1920 although a more detailed study of the
   beam-beam effects in the presence of a crossing angle would be
   desirable. The introduction of the crossing angle would result in a
   significant loss of luminosity for the relatively long bunch lengths
   (40-50 cm) currently planned for Run-II. An auxiliary high-frequency,
   high voltage system was proposed in TM-1920 to avoid excessive loss of
   luminosity. The 7-bucket spacing results in a relatively large number
   of interactions per beam crossing (9 at a luminosity of 1033). It is
   recognized that it would be desirable to be able reduce the number of
   interactions per crossing by decreasing the bunch spacing to a number
   significantly smaller than 7. However, the tune shift from the long
   range beam-beam interaction arising the near-miss collisions in the
   vicinity of the interaction region becomes more severe and may, in
   fact, be unmanageable. An alternative proposal to run at a less
   ambitious luminosity, limiting the interactions per crossing to,
   perhaps 5, appears to be feasible. The luminosity may be regulated by
   modulating the bunch length (through the rf Voltage), the crossing
   angle, the beam separation, or the low beta optics. All these methods
   work by changing the size (and shape) of the interaction region. The
   only known technical problem is the liklihood of producing unacceptably
   15 high beam losses for significant periods of time during the store.
   It is not known whether continuous or quasi-continuous changes in the
   parameters would be more or less effective in keeping the beam losses
   within an acceptable range. The intensity per bunch divided by the
   emittance (sometimes called "beam brightness") is limited by the
   maximum beam- beam tune shift. This limitation, until it is overcome by
   new techniques, limits the proton beam brightness to approximately the
   values proposed for Run-II (typical intensity of 30 \Theta 1010 protons
   per bunch with a normalized emittance of 20ss mm- mrad). However, the
   antiproton beam brightness is nearly an order of magnitude less in
   Run-II (typical intensity of 4 \Theta 1010 with a normalized emittance
   of 20ss mm-mrad) and could be increased more or less to the brightness
   of the proton beam if the anitproton stack size could be made large
   enough. More realistically, one could expect to increase the luminosity
   from 2 \Theta 1032 to 1 \Theta 1033 by increasing the number of bunches
   by a factor of 3 (or a bit less) and by increasing the antiproton bunch
   intensity by a factor of 2. Other methods of increasing the luminosity
   (changing the low beta optics, reducing the transverse beam size, or
   ameliorating the beam-beam tune shift limit) may be possible, may offer
   some advantages, and should be investigated more fully, but no method
   can circumvent the minimum requirements on the stacking rate. At the
   moment, however, the option of increasing the number of bunches and the
   antiproton bunch intensity seems to be both low cost and low risk.
   Antiprotons are produced by targeting 120 GeV protons: the antiproton
   production is increased by targeting more protons, expanding the
   antiproton collection acceptance, and decreasing the production cycle
   time. Insufficient work has been done to determine which of these
   options is the most attractive. The proton intensity is thought to be
   limited by the intensity available from the existing Booster. The
   replacement of the existing machine by a new "SuperBooster" holds the
   promise of circumventing the space charge tune shift limit in the
   Booster. This option is currently under study, but it may not be the
   most costeffective way to increase proton intensity for antiproton
   production. The accelerator physics that limits the Booster intensity
   is not well understood, so it is possible that the current Booster
   performance may be improved without a complete replacement of the
   existing Booster. For the moment, however, any Booster intensity
   improvements remain a matter of speculation. One might be able to
   circumvent the intensity limitations of a single Booster batch by
   stacking more than one Booster batch into the same azimuthal position
   in the Main Injector. In principle, one can consider stacking in
   transverse phase space or longitudinal phase space. A particular method
   of longitudinal phase space stacking (known as slip stacking) was
   proposed and similated for the Main Injector in conjunction with a
   proposed low-intensity polarized-proton source. Establishing the
   feasibility of stacking the higher intensity, larger emittance Booster
   beams that are used for antiproton production requires further study of
   the aperture limitations, 16 space charge tune shift effects, emittance
   growth during transition crossing, and possible coherent instabilities.
   The amount of rf power in the Main Injector is a known problem that can
   be solved by increasing the power delivered to the cavities, but
   further work is necessary to understand the effects of beam loading on
   the fundamental and higher order modes of the cavities. Another
   proposal that does not require higher intensity in the Main Injector is
   to target the full circumference of the Main Injector Ring instead of
   1=6 (a single Booster batch) as is currently planned for Run-II. A
   concept to utilize all of the Main Injector protons was discussed by
   Foster (FNAL-1902). Variants of this idea may be found in the
   Proceedings of the Indiana Workshop on Hadron-Hadron colliders
   (FNAL-1907). These ideas potentially conflict with Fermilab Main
   Injector fixedtarget programs such as NUMI. The extent of the conflict
   depends on the schedule for both Main Injector fixed target operations
   and for collider operation. One proposal is to share protons with NUMI
   by placing the antiproton production target upstream of NUMI target.
   The antiprotons are collected with a short focal length lithium lens,
   and dramatically reducing the thickness of the antiproton target is
   predicted to result in a minimal (10-20%) reduction in antiproton yield
   and a minimal reduction in the protons available to the NUMI target. A
   better understanding of the requirements of the NUMI beam (beam
   emittance and spill structure) and the efficiencies of the proposed
   bunch coalescing schemes will be major factors in assessing the
   attractiveness of these ideas. However one manages to target more
   protons, it is necessary to consider also the survivability of the
   target, lens, and other target station components. A preliminary
   investigation has not revealed any identifiable limitations beyond
   those imposed by limiting the target energy density below 700 J/g (to
   avoid local melting of the nickel target). This limitation can be
   circumvented with the sweeping system planned for Run-II and by
   defocussing the beam (resulting in a slight decrease in yield).
   Extensive experience working in this hostile and unforgiving
   environment suggests that more consideration needs to be given to the
   potential difficulties arising from the increased beam power and
   radiation that will be present in the target station in this scenario.
   Only a small fraction of the antiprotons produced in the target are
   available for use in the Tevatron. It is attractive to consider
   increasing the antiproton acceptance because the potential problems
   involved in targetting more protons can be avoided. It is possible to
   gain a factor of 2 in antiproton production by increasing the
   transverse phase space collection from 20ss to 40ss mm-mrad. Another
   factor of 2:5 could be achieved by increasing the momentum spread of
   the antiprotons from ffip=p from 4% to 10%. Methods for achieving
   higher acceptances range from a modification of the existing beam line
   and Debuncher (both the optics and the physical aperture) to a new high
   acceptance collection ring that would added to the
   Accumulator-Debuncher complex. One interesting concept involves the use
   of a linear Debuncher although this option may be somewhat expensive
   compared to the gain in momentum aperture. It has been noted that the
   Debuncher was built with straight vacuum chambers in a 17 curved
   dipole. In any scenario, the stochastic cooling of the antiprotons is
   an important issue that can not be avoided. Generally speaking, the
   antiproton flux that can be cooled is proportional to the cooling
   system bandwidth. Fortunately, it does not appear to be necessary to
   increase the typical 2-4 GHz bandwidth cooling system by a large factor
   to 10-20 GHz (currently beyond the state of the art). At least in some
   of the scenarios that have been considered the 4-8 GHz technology (with
   which we have some experience) could be used. A particularly attractive
   feature of the Recycler Ring is it can absorb some of the current
   momentum cooling load (four orders of magnitude in the Accumulator in
   Run-II) by electron cooling, stochastic cooling, or possibly some
   combination of the two. Although considerable detailed theoretical and
   experimental work is required before any option is sufficiently
   explored to propose implementation, a number of technically plausible
   ideas exist for achieving the luminosity goals of Tev33. The committee
   recommends an urgent study of the accelerator improvements required to
   reach the luminosities needed for a 30fb\Gamma 1 physics program
   following RunII. Innovative ideas should be explored to optimize useful
   luminosity for experiments at the lowest cost and disrupton to the
   physics program. Decisions on specific accelerator scenarios can have a
   dramatic impact on the viability of detectors and the physics potential
   of the TeV33 program. Close coordination of accelerator and detector
   upgrades will be an essential element of the studies required to
   achieve the full potential of the TeV33 program. 18
