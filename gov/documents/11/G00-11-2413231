http://nls3.nlm.nih.gov/MetaMap/mmi.desc.pdf

   MetaMap Indexing (MMI) Project Alan R. Aronson, LHNCBC/CgSB James
   Marcetich, LO/Index Toby G. Port, LO/MMS James G. Mork, VisionQuest
   Consulting July 10, 1997 The primary purpose of the MetaMap Indexing
   (MMI) project is to determine the usefulness of MetaMap processing
   applied to the task of automatic or semiautomatic indexing of the
   biomedi- cal literature. MetaMap discovers UMLS® Metathesaurus®
   concepts referred to in biomedical text such as the titles and
   abstracts of MEDLINE® citations. Concepts for a given citation are
   ranked by a function which emphasizes presence in the title and
   specificity of the concept. It is hoped that the ranked concepts can be
   used directly for retrieval purposes or indirectly as sug- gested terms
   during manual indexing. In addition concepts gleaned from users'
   queries can be selectively used to expand the query. Retrieval
   experiments on three test collections of MEDLINE citations show that
   using the auto- matically determined MMI-ranked concepts as indexing
   terms improves retrieval results by about 5%. Also using them to expand
   users' queries increases the improvement to about 8%. These results are
   about half as good as those obtained by using the MeSH field of the
   citations. Research efforts will be focused on closing the gap between
   the two methods. As an alternative to this com- pletely automatic
   approach, semiautomatic indexing experiments in which indexers are
   shown MMI-ranked concepts as suggested indexing terms are under way.
   Recently a prototype system for performing retrieval experiments using
   alternative methods of computing recommended indexing terms has been
   created. This Indexing Prototype initially has five indexing term
   computation methods. The Prototype allows for weighting these methods
   and computing recall/precision values for a small testset of two
   hundred MEDLINE citations. The MMI indexing method is the single best
   performer of the five methods. And the MMI method combined with a
   smaller weighting for John Wilbur's Related Citation method (ala
   PubMed) pro- duces the best results to date. Current research efforts
   concentrate both on exploring additional methods for recommending terms
   and on improved ranking of the recommended terms once they have been
   computed.
